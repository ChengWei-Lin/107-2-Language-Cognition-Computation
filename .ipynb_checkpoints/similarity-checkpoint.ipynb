{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob, re, os, nltk, math, string, numpy, urllib, pprint, operator\n",
    "path = os.getcwd()\n",
    "cand_li = os.listdir(path+\"\\data\")\n",
    "direc = []\n",
    "for cand in cand_li:\n",
    "    direc.append(os.listdir(path+\"\\data\\{}\\\\rawdata\".format(cand)))\n",
    "for a in range(len(direc)):\n",
    "    for b in range(len(direc[a])):\n",
    "        direc[a][b] = path+\"\\data\\\\\" + cand_li[a] + \"\\\\rawdata\\\\\" + direc[a][b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(direc, cand_var, doc_num):\n",
    "    print(\"cleaning candidate {}, document number {}\".format(cand_var, doc_num))\n",
    "    with open(direc[cand_var][doc_num], 'r', encoding='utf-8') as f:       \n",
    "        f = f.read()\n",
    "    f = f.lower()\n",
    "    f = f.replace('\\n', ' ')\n",
    "    f = f.replace('\"', '')\n",
    "    f = f.replace('\\'', '')\n",
    "    f = f.replace(',', '')\n",
    "    f = f.replace('.', '')\n",
    "    f = f.replace('?', '')\n",
    "    f = f.replace('`', '')\n",
    "    f = f.replace(';', '')\n",
    "    f = f.replace(':', '')\n",
    "    f = f.replace('(', '')\n",
    "    f = f.replace(')', '')\n",
    "    f = f.replace('_', '')\n",
    "    f = f.replace('$', '')\n",
    "    f = f.replace('-', '')\n",
    "    f = f.replace('@', '')\n",
    "    f = f.replace('%', '')\n",
    "    f = f.replace('&amp', '')\n",
    "    f = re.sub('\\d', '', f)\n",
    "    f = re.sub('\\s+', ' ', f)\n",
    "    #f = re.sub(r'\\b\\w\\b', ' ', f) # 剔除一個字的\n",
    "    f = f.split()\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see raw texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#texts = cleaning(direc, 1, 2)\n",
    "#print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  join texts together as data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning candidate 0, document number 0\n",
      "1 bags of words are now together\n",
      "cleaning candidate 0, document number 1\n",
      "2 bags of words are now together\n",
      "cleaning candidate 0, document number 2\n",
      "3 bags of words are now together\n",
      "cleaning candidate 0, document number 3\n",
      "4 bags of words are now together\n",
      "cleaning candidate 0, document number 4\n",
      "5 bags of words are now together\n",
      "cleaning candidate 0, document number 5\n",
      "6 bags of words are now together\n",
      "cleaning candidate 0, document number 6\n",
      "7 bags of words are now together\n",
      "cleaning candidate 0, document number 7\n",
      "8 bags of words are now together\n",
      "cleaning candidate 0, document number 8\n",
      "9 bags of words are now together\n",
      "cleaning candidate 0, document number 9\n",
      "10 bags of words are now together\n",
      "cleaning candidate 0, document number 10\n",
      "11 bags of words are now together\n",
      "cleaning candidate 0, document number 11\n",
      "12 bags of words are now together\n",
      "cleaning candidate 0, document number 12\n",
      "13 bags of words are now together\n",
      "cleaning candidate 0, document number 13\n",
      "14 bags of words are now together\n",
      "cleaning candidate 0, document number 14\n",
      "15 bags of words are now together\n",
      "cleaning candidate 0, document number 15\n",
      "16 bags of words are now together\n",
      "cleaning candidate 0, document number 16\n",
      "17 bags of words are now together\n",
      "cleaning candidate 0, document number 17\n",
      "18 bags of words are now together\n",
      "cleaning candidate 0, document number 18\n",
      "19 bags of words are now together\n",
      "cleaning candidate 0, document number 19\n",
      "20 bags of words are now together\n",
      "cleaning candidate 0, document number 20\n",
      "21 bags of words are now together\n",
      "cleaning candidate 0, document number 21\n",
      "22 bags of words are now together\n",
      "cleaning candidate 0, document number 22\n",
      "23 bags of words are now together\n",
      "cleaning candidate 0, document number 23\n",
      "24 bags of words are now together\n",
      "cleaning candidate 0, document number 24\n",
      "25 bags of words are now together\n",
      "cleaning candidate 1, document number 0\n",
      "26 bags of words are now together\n",
      "cleaning candidate 1, document number 1\n",
      "27 bags of words are now together\n",
      "cleaning candidate 1, document number 2\n",
      "28 bags of words are now together\n",
      "cleaning candidate 1, document number 3\n",
      "29 bags of words are now together\n",
      "cleaning candidate 1, document number 4\n",
      "30 bags of words are now together\n",
      "cleaning candidate 1, document number 5\n",
      "31 bags of words are now together\n",
      "cleaning candidate 1, document number 6\n",
      "32 bags of words are now together\n",
      "cleaning candidate 1, document number 7\n",
      "33 bags of words are now together\n",
      "cleaning candidate 1, document number 8\n",
      "34 bags of words are now together\n",
      "cleaning candidate 1, document number 9\n",
      "35 bags of words are now together\n",
      "cleaning candidate 1, document number 10\n",
      "36 bags of words are now together\n",
      "cleaning candidate 1, document number 11\n",
      "37 bags of words are now together\n",
      "cleaning candidate 1, document number 12\n",
      "38 bags of words are now together\n",
      "cleaning candidate 1, document number 13\n",
      "39 bags of words are now together\n",
      "cleaning candidate 1, document number 14\n",
      "40 bags of words are now together\n",
      "cleaning candidate 1, document number 15\n",
      "41 bags of words are now together\n",
      "cleaning candidate 1, document number 16\n",
      "42 bags of words are now together\n",
      "cleaning candidate 1, document number 17\n",
      "43 bags of words are now together\n",
      "cleaning candidate 1, document number 18\n",
      "44 bags of words are now together\n",
      "cleaning candidate 1, document number 19\n",
      "45 bags of words are now together\n",
      "cleaning candidate 1, document number 20\n",
      "46 bags of words are now together\n",
      "cleaning candidate 1, document number 21\n",
      "47 bags of words are now together\n",
      "cleaning candidate 2, document number 0\n",
      "48 bags of words are now together\n",
      "cleaning candidate 2, document number 1\n",
      "49 bags of words are now together\n",
      "cleaning candidate 2, document number 2\n",
      "50 bags of words are now together\n",
      "cleaning candidate 2, document number 3\n",
      "51 bags of words are now together\n",
      "cleaning candidate 2, document number 4\n",
      "52 bags of words are now together\n",
      "cleaning candidate 2, document number 5\n",
      "53 bags of words are now together\n",
      "cleaning candidate 2, document number 6\n",
      "54 bags of words are now together\n",
      "cleaning candidate 2, document number 7\n",
      "55 bags of words are now together\n",
      "cleaning candidate 2, document number 8\n",
      "56 bags of words are now together\n",
      "cleaning candidate 2, document number 9\n",
      "57 bags of words are now together\n",
      "cleaning candidate 2, document number 10\n",
      "58 bags of words are now together\n",
      "cleaning candidate 2, document number 11\n",
      "59 bags of words are now together\n",
      "cleaning candidate 2, document number 12\n",
      "60 bags of words are now together\n",
      "cleaning candidate 2, document number 13\n",
      "61 bags of words are now together\n",
      "cleaning candidate 2, document number 14\n",
      "62 bags of words are now together\n",
      "cleaning candidate 2, document number 15\n",
      "63 bags of words are now together\n",
      "cleaning candidate 2, document number 16\n",
      "64 bags of words are now together\n",
      "cleaning candidate 2, document number 17\n",
      "65 bags of words are now together\n",
      "cleaning candidate 2, document number 18\n",
      "66 bags of words are now together\n",
      "cleaning candidate 2, document number 19\n",
      "67 bags of words are now together\n",
      "cleaning candidate 2, document number 20\n",
      "68 bags of words are now together\n",
      "cleaning candidate 2, document number 21\n",
      "69 bags of words are now together\n",
      "cleaning candidate 2, document number 22\n",
      "70 bags of words are now together\n",
      "cleaning candidate 3, document number 0\n",
      "71 bags of words are now together\n",
      "cleaning candidate 3, document number 1\n",
      "72 bags of words are now together\n",
      "cleaning candidate 3, document number 2\n",
      "73 bags of words are now together\n",
      "cleaning candidate 3, document number 3\n",
      "74 bags of words are now together\n",
      "cleaning candidate 3, document number 4\n",
      "75 bags of words are now together\n",
      "cleaning candidate 3, document number 5\n",
      "76 bags of words are now together\n",
      "cleaning candidate 3, document number 6\n",
      "77 bags of words are now together\n",
      "cleaning candidate 3, document number 7\n",
      "78 bags of words are now together\n",
      "cleaning candidate 3, document number 8\n",
      "79 bags of words are now together\n",
      "cleaning candidate 3, document number 9\n",
      "80 bags of words are now together\n",
      "cleaning candidate 3, document number 10\n",
      "81 bags of words are now together\n",
      "cleaning candidate 3, document number 11\n",
      "82 bags of words are now together\n",
      "cleaning candidate 3, document number 12\n",
      "83 bags of words are now together\n",
      "cleaning candidate 3, document number 13\n",
      "84 bags of words are now together\n",
      "cleaning candidate 3, document number 14\n",
      "85 bags of words are now together\n",
      "cleaning candidate 3, document number 15\n",
      "86 bags of words are now together\n",
      "cleaning candidate 3, document number 16\n",
      "87 bags of words are now together\n",
      "cleaning candidate 3, document number 17\n",
      "88 bags of words are now together\n",
      "cleaning candidate 3, document number 18\n",
      "89 bags of words are now together\n",
      "cleaning candidate 3, document number 19\n",
      "90 bags of words are now together\n",
      "cleaning candidate 3, document number 20\n",
      "91 bags of words are now together\n",
      "cleaning candidate 3, document number 21\n",
      "92 bags of words are now together\n",
      "cleaning candidate 3, document number 22\n",
      "93 bags of words are now together\n",
      "cleaning candidate 3, document number 23\n",
      "94 bags of words are now together\n",
      "cleaning candidate 3, document number 24\n",
      "95 bags of words are now together\n",
      "cleaning candidate 3, document number 25\n",
      "96 bags of words are now together\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "bag_of_words = []\n",
    "count = 0\n",
    "for a in range(len(cand_li)):\n",
    "    for b in range(len(direc[a])):\n",
    "        count += 1\n",
    "        words = cleaning(direc, a, b)\n",
    "        bag_of_words.append(words)\n",
    "        print(\"{} bags of words are now together\".format(count))\n",
    "    \n",
    "\n",
    "with open('data.txt', 'w', encoding=\"utf-8\") as f:\n",
    "    for text in bag_of_words:\n",
    "        output = ' '.join(text)\n",
    "        f.write(output)\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Gensim to Train Model\n",
    "input data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qqoao\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training. Duration: 18.282942163944245mins\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "sentences = word2vec.LineSentence('data.txt')\n",
    "model = word2vec.Word2Vec(sentences, size=250)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Finished Training. Duration: {}mins\".format((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88286483"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('donald', 'republican')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92231166"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('donald', 'democrat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8452647"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('trump', 'republican')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8732928"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('trump', 'democrat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('donald', 0.9736670851707458),\n",
       " ('bernie', 0.9593716859817505),\n",
       " ('hillary', 0.9554997086524963),\n",
       " ('mr', 0.9548900723457336),\n",
       " ('oh', 0.9489075541496277),\n",
       " ('name', 0.9484243988990784),\n",
       " ('clinton', 0.9480516314506531),\n",
       " ('sanders', 0.9417569041252136),\n",
       " ('cochran', 0.9414020776748657),\n",
       " ('shell', 0.940734326839447),\n",
       " ('wondering', 0.9385011196136475),\n",
       " ('surprised', 0.9367485642433167),\n",
       " ('terrible', 0.9364933371543884),\n",
       " ('darling', 0.9312797784805298),\n",
       " ('resign', 0.9287402033805847),\n",
       " ('id', 0.9287379384040833),\n",
       " ('brilliant', 0.9278153777122498),\n",
       " ('glenn', 0.9274150133132935),\n",
       " ('myself', 0.9266871809959412),\n",
       " ('ridiculous', 0.9265809059143066),\n",
       " ('sean', 0.9261304140090942),\n",
       " ('answer', 0.9241859912872314),\n",
       " ('tone', 0.9236507415771484),\n",
       " ('general', 0.9236419200897217),\n",
       " ('uk', 0.9219269752502441),\n",
       " ('argument', 0.9216655492782593),\n",
       " ('chris', 0.9214514493942261),\n",
       " ('stone', 0.9204974174499512),\n",
       " ('alright', 0.919576108455658),\n",
       " ('rubio', 0.9192413687705994),\n",
       " ('kinda', 0.9180744886398315),\n",
       " ('wright', 0.9177824854850769),\n",
       " ('morning', 0.9169022440910339),\n",
       " ('i’ve', 0.916468620300293),\n",
       " ('mcquaid', 0.9162611365318298),\n",
       " ('debate', 0.9145548343658447),\n",
       " ('favor', 0.9135188460350037),\n",
       " ('marco', 0.9130404591560364),\n",
       " ('socialist', 0.9129528403282166),\n",
       " ('honestly', 0.9129334092140198),\n",
       " ('limbaugh', 0.9124888777732849),\n",
       " ('dr', 0.9122951030731201),\n",
       " ('reuters', 0.911861002445221),\n",
       " ('jfk', 0.9108797907829285),\n",
       " ('congratulations', 0.9108758568763733),\n",
       " ('clerk', 0.9099491834640503),\n",
       " ('hated', 0.90959632396698),\n",
       " ('press', 0.909570574760437),\n",
       " ('brother', 0.9095075726509094),\n",
       " ('underestimate', 0.909343421459198),\n",
       " ('carson', 0.9089978933334351),\n",
       " ('unhappy', 0.9088567495346069),\n",
       " ('expression', 0.9088082313537598),\n",
       " ('perry', 0.9088002443313599),\n",
       " ('probably', 0.9087173342704773),\n",
       " ('story', 0.9079681038856506),\n",
       " ('speech', 0.9078270196914673),\n",
       " ('naked', 0.9067900776863098),\n",
       " ('apprentice', 0.9066282510757446),\n",
       " ('point', 0.9056951403617859),\n",
       " ('honored', 0.9049438834190369),\n",
       " ('george', 0.9048315286636353),\n",
       " ('wow', 0.9034489393234253),\n",
       " ('christie', 0.9033601880073547),\n",
       " ('liked', 0.9032694697380066),\n",
       " ('mitch', 0.9027583003044128),\n",
       " ('believed', 0.902725100517273),\n",
       " ('quit', 0.9023292064666748),\n",
       " ('likes', 0.9023063778877258),\n",
       " ('bush', 0.9018129110336304),\n",
       " ('favorite', 0.9008042812347412),\n",
       " ('cleveland', 0.9007501006126404),\n",
       " ('barack', 0.9004712700843811),\n",
       " ('‘if', 0.9004145860671997),\n",
       " ('upset', 0.899978518486023),\n",
       " ('hi', 0.8996074199676514),\n",
       " ('conference', 0.89959716796875),\n",
       " ('cnn', 0.8995862007141113),\n",
       " ('hearings', 0.8989304304122925),\n",
       " ('mom', 0.8982115983963013),\n",
       " ('gray', 0.8980991244316101),\n",
       " ('la', 0.8980361223220825),\n",
       " ('grey', 0.8980294466018677),\n",
       " ('newt', 0.8977039456367493),\n",
       " ('argue', 0.8976059556007385),\n",
       " ('critics', 0.8974831104278564),\n",
       " ('usually', 0.897275984287262),\n",
       " ('senator', 0.8968818187713623),\n",
       " ('interview', 0.8965904712677002),\n",
       " ('book', 0.8965019583702087),\n",
       " ('loud', 0.8959600329399109),\n",
       " ('wife', 0.8953897356987),\n",
       " ('summer', 0.8952726125717163),\n",
       " ('joe', 0.8947396874427795),\n",
       " ('kerry', 0.8947017192840576),\n",
       " ('yesterday', 0.8945282101631165),\n",
       " ('impressed', 0.8943281173706055),\n",
       " ('sounds', 0.8942114114761353),\n",
       " ('stops', 0.8936852812767029),\n",
       " ('statement', 0.8932645916938782)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word('trump', topn=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
